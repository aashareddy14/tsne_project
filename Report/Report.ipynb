{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of Visualizing Data Using t-SNE\n",
    "\n",
    "#### Aasha Reddy and Madeleine Beckner\n",
    "\n",
    "#### Due: April 27, 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Abstract\n",
    "\n",
    "250 words or less. Identify 4-6 key phrases.\n",
    "\n",
    "t-SNE. In this report, we will implement the t-SNE algorithm as described in the paper Visualizing Data using t-SNE (Hinton and van der Maaten, 2008). We will demonstrate its accuracy and performance through various applications to both real and simulated datasets, optimize the code, and compare this algorithm with two other competing algorithms that address a similar problem.\n",
    "\n",
    "Key phrases: t-SNE, stochastic neighbor embedding, dimensionality reduction, visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Background\n",
    "\n",
    "State the research paper you are using. Describe the concept of the algorithm and why it is interesting and/or useful. If appropriate, describe the mathematical basis of the algorithm. Some potential topics for the background include:\n",
    "What problem does it address?\n",
    "What are known and possible applications of the algorithm?\n",
    "What are its advantages and disadvantages relative to other algorithms?\n",
    "How will you use it in your research?\n",
    "\n",
    "In this paper, we will implement the t-SNE algorithm for visualizing high-dimensional data as described in the paper Visualizing Data using t-SNE (Hinton and van der Maaten, 2008). This algorithm gives each datapoint a corresponding location in a two or three-dimensional mapping. The t-SNE algorithm improves upon the Stochastic Neighbor Embedding algorithm (Hinton and Roweis, 2002) by reducing issues with points crowding in the center of the map.\n",
    "\n",
    "Visualizing high-dimensional data in an effective and interpretable way is important to many different areas of application. High-dimensional datasets are common in numerous fields of study, such as biology, chemistry, political science, economics, astronomy, physics, and more. As availability of computing resources continues to increase, so does the ability to collect and store more and more complex and high-dimensional datasets. Effective visualization  presents an interesting and important challenge in analyzing and understanding these complex datasets.\n",
    "\n",
    "Many different high-dimensional data methods have been proposed in recent years, but many of these methods come with certain issues. For example, techniques that use iconographic displays such as pixel-based techniques or Chernoff faces provide tools to visualize high-dimensional data while leaving the interpretation to the viewer. When scaled to thousands of dimensions, as is often the case with real-world data, these methods break down due to human inability to interpret this kind of visual dimension scale. To avoid this issue, methods with dimensionality-reduction have been proposed that convert high-dimensional data into two or three dimensions to be plotted. Popular linear dimensionality reduction methods include PCA, or Principal Components Analysis, and MDS, Multi-Dimensional Scaling. However, issues arise in applications where low-dimensional representations of similar points must be kept close together, because these methods prioritize keeping low-dimensional representations of dissimilar points far apart. To avoid this concern and preserve local data structures, non-linear dimensionality reduction methods have been proposed as well, one of which, Stochastic Neighbor Embedding, forms the basis of this paper. While these types of methods improve over other mentioned concerns and generally perform well on artificial datasets, they tend to break down when introduced to real-world high dimensional datasets because they are not capable of retaining both local and the global structure of the data in a single map. The method t-SNE presented in this paper provides a solution by being capable of capturing both local and global structures effectively. t-SNE has many advantages relative to other algorithms as discussed, and forms a valuable tool in effectively visualizing high-dimensional data. t-SNE has been shown to be better than existing techniques at creating a single map that reveals structure at many different scales.\n",
    "\n",
    "While t-SNE makes vast improvements in comparison to other existing methods, one should note that this algorithm has its own weaknesses as well. First, it is not clear how t-SNE performs on general dimensionality reduction tasks where d > 3 due to the heavy tails of the Student-t distribution. Additionally, the relatively local nature of t-SNE makes it sensitive to the curse of the intrinsic dimensionality of the data, which means t-SNE might be less successful if it is applied on data sets with a very high intrinsic dimensionality. The most significant potential weakness is that t-SNE is not guaranteed to converge to a global optimum of its cost function due to the non-convexity of the cost function. Therefore, we have to choose several optimization parameters, and the solutions may differ depending on the choice of these parameters. In spite of these drawbacks, t-SNE still outperforms other similar methods and improves upon common issues in high-dimensional visualization methods. In our paper, we will implement and evaluate the t-SNE metric on multiple datasets. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. t-SNE Algorithm\n",
    "\n",
    "First, explain in plain English what the algorithm does. Then describes the details of the algorithm, using mathematical equations or pseudocode as appropriate.\n",
    "\n",
    "t-SNE is a variation of Stochastic Neighbor Embedding algorithm (Hinton and Roweis, 2002) that simplifies optimization and prevents crowding in the center of the map (also referred to as the crowding problem). In order to understand t-SNE, we must first discuss Stochastic Neighbor Embedding (SNE), which forms a basis for the t-SNE algorithm.\n",
    "\n",
    "### 2.1 Stochastic Neighbor Embedding (SNE)\n",
    "\n",
    "SNE first converts Euclidean distances in high dimensions between the data into conditional probabilities, or pairwise affinities/similarities. The conditional probability as shown below represents a similarity between the two datapoints. In other words, $p_{j \\mid i}$ is the probability that datapoint $x_i$ would choose $x_j$ as its closest point if closest points were chosen in proportion to their probability density under a Normal distribution centered at $x_i$. Therefore, when datapoints are close, the conditional probability will be low (and vice versa).\n",
    "\n",
    "The conditional probability can be written mathematically as follows:\n",
    "$$p_{j \\mid i} = \\frac{\\text{exp}(-\\lVert x_i - x_j \\rVert^2 / 2\\sigma_i^2)}{\\sum_{k\\neq i} \\text{exp}(-\\lVert x_i - x_k \\rVert^2 / 2\\sigma_i^2) }$$\n",
    "\n",
    "The value of $\\sigma_i$ is tricky because it must be determined by binary search with a fixed perplexity defined by the user, typically between 5 and 50.\n",
    "\n",
    "$$\\text{Perplexity} = 2^{- \\sum_j p_{j \\mid i} \\text{log}_2 p_{j \\mid i}}$$\n",
    "\n",
    "Next in SNE, we calculate a similar probability for low-dimensional counterparts of the high-dimensional points $x_i$ and $x_j$. This time the variance is set to $\\frac{1}{\\sqrt2}$:\n",
    "\n",
    "$$q_{j \\mid i} = \\frac{\\text{exp}(-\\lVert y_i - y_j \\rVert^2 )}{\\sum_{k\\neq i} \\text{exp}(-\\lVert y_i - y_k \\rVert^2) }$$ \n",
    "\n",
    "These two conditional probabilities will be equal if the map correctly models the similarity between datapoints in high dimensions and low dimensions. Therefore, SNE works by minimizing the difference between $q_{j \\mid i}$ and $p_{j \\mid i}$ in finding the value for $q_{j \\mid i}$, the low dimensional counterpoint. This difference is minimized in SNE using gradient descent with a KL divergence:\n",
    "\n",
    "$$ \\text{Cost for SNE} = \\sum_i \\sum_j p_{j \\mid i} \\text{log} \\frac{p_{j \\mid i}}{q_{j \\mid i}} $$\n",
    "\n",
    "This cost function is large for using far datapoints to represent near ones, but it is relatively small when using close datapoints to represent far ones. In this way, SNE uses an asymmetrical cost function to preserve local map structure.\n",
    "\n",
    "The minimization of the cost above is performed with gradient descent. The gradient is defined here:\n",
    "\n",
    "$$\\frac{\\delta C}{\\delta y_i} = 2 \\sum_j (p_{j \\mid i} -  q_{j \\mid i} + p_{i \\mid j} - q_{i \\mid j})(y_i - y_j)$$\n",
    "\n",
    "### 2.2 t-SNE and Comparison\n",
    "\n",
    "t-SNE is very similar to SNE, but differs in two main waysâ€“ it uses a different (symmetric) cost function, and a Student-t distribution to compute similarity in low-dimension. The heavy tail of the Student t-distribution alleviates the crowding problem and difficult optimization from the SNE algorithm.\n",
    "\n",
    "We define the probabilities differently to use the symmetric cost function. We will be using joint probabilities as follows (due to the properties of the Student t-distribution as well as symmetric cost function):\n",
    "\n",
    "\n",
    "$$q_{ij} = \\frac{(1 + \\lVert y_i - y_j \\rVert^2 )^{-1}}{\\sum_{k\\neq l} (1 + \\lVert y_k - y_l \\rVert^2)^{-1} }$$ \n",
    "\n",
    "$$p_{ij} = \\frac{p_{j \\mid i} + p_{i \\mid j}}{2n}$$ \n",
    "\n",
    "The Perplexity is defined the same as in SNE: $$\\text{Perplexity} = 2^{- \\sum_j p_{j \\mid i} \\text{log}_2 p_{j \\mid i}}$$\n",
    "\n",
    "\n",
    "t-SNE differs from SNE in choice of cost function. The cost function for t-SNE is symmetric and has simpler gradients:\n",
    "\n",
    "$$ \\text{Cost for t-SNE} = \\sum_i \\sum_j p_{ij} \\text{log} \\frac{p_{ij}}{q_{ij}} $$\n",
    "\n",
    "\n",
    "The minimization of the cost above is performed with gradient descent using momentum. The gradient is defined here (simpler than that of SNE):\n",
    "\n",
    "$$\\frac{\\delta C}{\\delta y_i} = 4 \\sum_j (p_{ij} -  q_{ij})(y_i - y_j) (1 + \\lVert y_i - y_j \\rVert^2)^{-1}$$\n",
    "\n",
    "\n",
    "Gradient descent using momentum allows us to speed up the optimization and to avoid poor local minima. This type of gradient descent remembers the update at each iteration and determines the next update as a linear combination of the gradient and the previous update. The update function is given below, where $\\eta$ is defined to be the learning rate and $\\alpha$ is the momentum. \n",
    "\n",
    "$$Y^t = Y^{t-1} + \\eta \\frac{\\delta C}{\\delta y_i} + \\alpha (Y^{t-1} - Y^{t-2})$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 t-SNE Pseudocode\n",
    "\n",
    "1. Begin with a data matrix $X$, and a chosen perplexity level (default to 30)\n",
    "2. Compute pairwise distance matrix $D = \\lVert x_i - x_j \\rVert^2 $, using squared euclidean distance\n",
    "3. Compute pairwise similarity matrix (conditional probabilities) $p_{j|i}$, using $D$ and binary search to find the optimal $\\sigma_i$\n",
    "3. Calculate joint probabilities $p_{ij}$ \n",
    "5. Sample initial solution $Y^{0} = (Y_1, Y_2)$ from Normal(0, 0.0001)\n",
    "    6. For i in 1:max_iterations\n",
    "        7. compute low-dimensional affinities $q_{ij}$\n",
    "        8. compute gradient $\\frac{\\delta C}{\\delta y_i}$\n",
    "        9. Update $Y$: $Y^t = Y^{t-1} + \\eta \\frac{\\delta C}{\\delta y_i} + \\alpha (Y^{t-1} - Y^{t-2})$\n",
    "\n",
    "\n",
    "\n",
    "5. Result is $Y$, a 2-dimensional data representation of $X$: $Y = (Y_1, Y_2)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Python Implementation - EXPLAIN SOME BACKGROUND ABOUT PYTHON LIBRARY AND WHAT IT INCLUDES\n",
    "\n",
    "Our implementation of t-SNE can be found in the library **INSERT NAME OF PYTHON LIBRARY**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Optimization for Performance\n",
    "\n",
    "First implement the algorithm using plain Python in a straightforward way from the description of the algorithm. Then profile and optimize it using one or more appropriate methods, such as:\n",
    "\n",
    "1. Use of better algorithms or data structures\n",
    "2. Use of vectorization\n",
    "3. JIT or AOT compilation of critical functions\n",
    "4. Re-writing critical functions in C++ and using pybind11 to wrap them\n",
    "5. Making use of parallelism or concurrency\n",
    "6. Making use of distributed compuitng\n",
    "\n",
    "Document the improvement in performance with the optimizations performed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Profiling\n",
    "\n",
    "We tested our initial implementation of t-SNE on the MNIST dataset. The MNIST dataset is comprised of 60,000 grayscale images of handwritten digits, and we test on a 1000-observation subset. Profiled results our initial implementation can be found below. We show the top ten functions ordered by cumulative time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Apr 24 18:00:18 2021    tsne.prof\n",
      "\n",
      "         1910406 function calls in 1163.256 seconds\n",
      "\n",
      "   Ordered by: cumulative time\n",
      "   List reduced from 61 to 10 due to restriction <10>\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "        1    0.000    0.000 1163.256 1163.256 {built-in method builtins.exec}\n",
      "        1    0.000    0.000 1163.256 1163.256 <string>:1(<module>)\n",
      "        1    2.048    2.048 1163.256 1163.256 <ipython-input-55-17c53d1c44a4>:118(tsne)\n",
      "     1000  951.282    0.951  977.286    0.977 <ipython-input-55-17c53d1c44a4>:104(grad_C)\n",
      "     1000  165.805    0.166  172.470    0.172 <ipython-input-55-17c53d1c44a4>:72(q_ij)\n",
      "   177196   20.150    0.000   20.150    0.000 {method 'reduce' of 'numpy.ufunc' objects}\n",
      "    59399    0.169    0.000   19.003    0.000 {method 'sum' of 'numpy.ndarray' objects}\n",
      "    59399    0.141    0.000   18.833    0.000 /opt/conda/lib/python3.6/site-packages/numpy/core/_methods.py:36(_sum)\n",
      "     2001    0.035    0.000   13.753    0.007 <ipython-input-55-17c53d1c44a4>:1(squared_euc_dist)\n",
      "        1    0.008    0.008   11.452   11.452 <ipython-input-55-17c53d1c44a4>:84(p_ij)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pstats\n",
    "p = pstats.Stats('tsne.prof')\n",
    "p.sort_stats('cumulative').print_stats(10)\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that our tsne function, which uses binary search to find the optimal $\\sigma's$ to calculate the $p_{ij}$ matrix takes the longest amount of time. We also note that our tsne function calls the function that calculates $q_{ij}$, the gradient (grad_C) as well, so we would like to optimize all of these functions if possible. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Optimization using Cython and Numba JIT\n",
    "\n",
    "We first optimize performance by implementing our functions in Cython and JIT (just-in-time compiler, from the Python package `numba`). Cython converts Python to C code, while JIT optimizes machine code from the LLVM compiler. To implement our functions, we note that some functions need to be re-written to avoid use of certain Python libraries which are incompatible with Numba. We find that the Cythonized implementation is slower than our un-optimized code, due to high overhead costs of type conversion, while our JIT implementation performed about 4.6 times faster (see below table for performance comparison). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Futher Optimize using Explicit Loops and PCA Initialization\n",
    "\n",
    "We are able to further optimize our code using explicit loops in the calculation of a pairwise distance matrix, $D$. This distance matrix is an input for our calculation of the $p_{ij}$ matrix of pairwise affinities. We had initially vectorized this calculation using `numpy` operations, and then optimized for performance by forgoing vectorization and instead defining explicit loops. This makes it even easier for the `numba` JIT to optimize our implementation. We see a good speed-up here from about 4.6 times faster to 5.2 times faster than our original implementation. We choose to definite explit loops in our `squared_euc_dist` function in our final implementation for our package.\n",
    "\n",
    "The user of our package is able to further optimize performance by performing PCA on the data to initialize before using our t-SNE function. This allows for an initial reduction in dimensions rather quickly, before further reduction is done using t-SNE. PCA is in general much faster than t-SNE, and initialization with PCA can speed up computation of pairwise distances and supresses some noise without severe distortion of distances between points. We can see a slight increase in performance on top of including explicit looping for distance calculations. We choose to leave PCA initialization out of our final package, and as a choice for the user to implement based on background knowledge of their data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Comparison Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below table shows the Speed-up Multiplier of our optimization compared to our initial implementation of t-SNE. All tests were perfomed on MNSIT data as described above. In the end we use the Numba JIT implementation of our t-SNE algorithm with Looped Distance, leaving PCA initialization as up to the user of our package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Implementation</th>\n",
       "      <th>Speed-up Multipler</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Numba</td>\n",
       "      <td>4.605476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cython</td>\n",
       "      <td>0.700209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PCA Initialized Numba</td>\n",
       "      <td>4.992868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Numba with Looped Distance</td>\n",
       "      <td>5.213304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PCA Initialized Numba with Looped Distance</td>\n",
       "      <td>5.291994</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Implementation  Speed-up Multipler\n",
       "0                                       Numba            4.605476\n",
       "1                                      Cython            0.700209\n",
       "2                       PCA Initialized Numba            4.992868\n",
       "3                  Numba with Looped Distance            5.213304\n",
       "4  PCA Initialized Numba with Looped Distance            5.291994"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "speed = pd.read_csv('Report_Plots/speed_table_final.csv', header = 0, names = ['Implementation', 'Speed-up Multipler'])\n",
    "speed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Testing on Datasets from Original Paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike PCA, t-SNE is not deterministic. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applications to Simulated Datasets\n",
    "\n",
    "Are there specific inputs that give known outputs (e.g. there might be closed form solutions for special input cases)? How does the algorithm perform on these? \n",
    "\n",
    "If no such input cases are available (or in addition to such input cases), how does the algorithm perform on simulated data sets for which you know the \"truth\"? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Applications to Real Datasets\n",
    "\n",
    "Test the algorithm on the real-world examples in the original paper if possible. Try to find at least one other real-world data set not in the original paper and test it on that. Describe and interpret the results.\n",
    "\n",
    "\n",
    "#### MNIST dataset\n",
    "\n",
    "We tested our original algorithm on the MNIST dataset, .\n",
    "Here is the plot run on the paper author's code implementation, provided on his website.\n",
    "![img](Report_Plots/paper_MNIST_plot.png)\n",
    "\n",
    "Here is the plot run on our implementation:\n",
    "\n",
    "![img2](Report_Plots/our_MNIST_plot.png)\n",
    "\n",
    "#### IRIS dataset (small dataset for original testing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Comparative Analysis to Competing Algorithms\n",
    "\n",
    "Find two other algorithm that address a similar problem. Perform a comparison - for example, of accuracy or speed. You can use native libraries of the other algorithms - you do not need to code them yourself. Comment on your observations. \n",
    "\n",
    "### Comparison: PCA\n",
    "![img3](Report_Plots/PCA_MNIST_plot.png)\n",
    "\n",
    "\n",
    "Overall PCA is much faster. However, it also does a much worse job of visualizing the data as it is difficult to see the patters between the different labels.\n",
    "\n",
    "### Comparison: Isomap\n",
    "\n",
    "![img4](Report_Plots/Isomap_MNIST_plot.png)\n",
    "\n",
    "\n",
    "This method (non-linear dimension reduction) is also faster than t-SNE as expected, but slower than linear dimension reduction. It is an improvement to linear dimension reduction in the effectiveness of the visualization, but it is not as effective as t-SNE or more effective methods.\n",
    "\n",
    "### Comparison: Locally Linear Embedding\n",
    "\n",
    "![img5](Report_Plots/LLE_MNIST_plot.png)\n",
    "\n",
    "\n",
    "\n",
    "### Comparison: Neighborhood Components Analysis\n",
    "\n",
    "![img6](Report_Plots/NCA_MNIST_plot.png)\n",
    "\n",
    "\n",
    "\n",
    "### Comparison: MDS\n",
    "\n",
    "![img7](Report_Plots/MDS_MNIST_plot.png)\n",
    "\n",
    "\n",
    "## ADD SPEED TABLE AND COMPARE\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Discussion and Conclusion - Explain Challenges like large D matrix and not converging to find optimal sigmas\n",
    "\n",
    "Your thoughts on the algorithm. Does it fulfill a particular need? How could it be generalized to other problem domains? What are its limitations and how could it be improved further?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1. van der Maaten, Laurens and Hinton, Geoffrey. \"Visualizing Data using t-SNE .\" Journal of Machine Learning Research 9 (2008): 2579--2605.\n",
    "2. Sci-kit Learn implementation: https://github.com/scikit-learn/scikit-learn/blob/95119c13af77c76e150b753485c662b7c52a41a2/sklearn/manifold/_utils.pyx\n",
    "3. http://www.sci.utah.edu/~beiwang/publications/Vis_HD_STAR_BeiWang_2015.pdf\n",
    "4. https://towardsdatascience.com/dimensionality-reduction-using-t-distributed-stochastic-neighbor-embedding-t-sne-on-the-mnist-9d36a3dd4521\n",
    "5. https://scikit-learn.org/stable/modules/generated/sklearn.manifold.Isomap.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
